---
title: "ReadMe"
author: "~PsychBrief"
date: "October 22, 2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The full data set without any alterations by me is in the file called "Full non-transformed data" (available at: https://osf.io/kty48/). All of the data is nominal except for the answers for question 9 (where respondents rated how relevant each reason was for leaving academia) which is ordinal. To analyse it in R, I combined the two headers into one row so there was only one row of questions. I then removed all the participants who had left question 9 blank or participants who were giving joke answers (only two such participants were removed). I transformed the answers for Q9 from words (e.g. "Not relevant", "Highly relevant" etc.) to numbers (0 for a blank answer, 1 for "Not relevant", 2 for "Slightly relevant", 3 for "Moderately relevant", 4 for "Highly relevant", and 5 for "Extremely relevant"). This was then saved as "MyDataReport_(with_exclusions).csv". No columns/questions were removed. 

## Graph for the mean scores for question 9 ##
To create the graph for the weighted average results for question 9 (after exclusions), I created a number vector of the means for each variable for question 9 (the reasons for leaving academia) and a character vector for those reasons. I combined them into a data frame and then forced the order of the labels for the character vector. I then created my bar graph by adding bars, colouring them, and adding the mean of the reason on the graph.

```{r, echo=TRUE, fig.width=15, fig.height=6}
#Create a number vector of the scores
Reasons_scores<-c(3.25,3.43,2.8,2.19,1.99,2.81,1.75,1.8,3.54,2.66,2.39,3.18,3.51)
#Create a character vector of the options
Reasons<-c("Stress", "W_L_Balance", "Truth_of_field", "Criticism_culture", "Bullying", "Pay", "Criticised", "Replics", "Incentives", "No_benefit", "No_progress", "Location", "Funding")
#create a data frame of the reasons and reasons scores
ReasonsData<-data.frame(Reasons,Reasons_scores)
#Force the order of the lables for the character vector Reasons
ReasonsData$Reasons=factor(ReasonsData$Reasons, levels=c("Stress", "W_L_Balance", "Truth_of_field", "Criticism_culture", "Bullying", "Pay", "Criticised", "Replics", "Incentives", "No_benefit", "No_progress", "Location", "Funding"))
#Graph object of the data frame with Reasons as the x axis and Reasons_scores as the y
library(ggplot2)
Reasons_graph<-ggplot(ReasonsData,aes(Reasons,Reasons_scores))
#Put bars on my graph object, fill in the bars in blue, give it a title and centre it, put the scores on the bars, change the colour of the label font to white, and change the size of the font for the axis labels.
Reasons_graph+geom_bar(stat="identity",fill = "dodgerblue2")+ ggtitle("Weighted average of reasons for leaving academia (rated from 1-5)")+ theme(plot.title = element_text(hjust = 0.5))+ geom_text(aes(label = Reasons_scores), colour="white", size = 5, hjust = 0.5, vjust = 3, position =     "stack")+ theme(text = element_text(size=14))
```

After uploading the data (including exclusions) to GitHub I downloaded it into R and turned the integer values of question 9 into a factor with labels corresponding with the answer options from the survey.

```{r, echo=TRUE}
library(readr, knitr)
GitHub_data<-"https://raw.githubusercontent.com/PsychBrief/Why-do-psychologists-leave-academia/master/MyDataReport_(with_exclusions).csv"
QuestionnaireData<-read.csv(GitHub_data)
names(QuestionnaireData)<-c("Consent","Age","Work_Location","Sex","Occupation","Other1","Academic?","PhD","Postdoc","Assistant_Lecturer","Lecturer","Senior_Lecturer","Reader","Professor","Trainee_Prac_Psych","Prac_Psych","Other2","Left_Academia?","Stress","W_L_balance","Truth_of_field","Criticism_culture","Bullying","Pay","Criticised","Replics","Incentives","No_benefit","No_progress","Location","Funding","Other3","Comments")
QuestionnaireData$Stress<-factor(QuestionnaireData$Stress,labels=c("Blank", "Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$W_L_balance<-factor(QuestionnaireData$W_L_balance,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Truth_of_field<-factor(QuestionnaireData$Truth_of_field,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Criticism_culture<-factor(QuestionnaireData$Criticism_culture,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Bullying<-factor(QuestionnaireData$Bullying,labels=c("Blank", "Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Pay<-factor(QuestionnaireData$Pay,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Criticised<-factor(QuestionnaireData$Criticised,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Replics<-factor(QuestionnaireData$Replics,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Incentives<-factor(QuestionnaireData$Incentives,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$No_benefit<-factor(QuestionnaireData$No_benefit,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$No_progress<-factor(QuestionnaireData$No_progress,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Location<-factor(QuestionnaireData$Location,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
QuestionnaireData$Funding<-factor(QuestionnaireData$Funding,labels=c("Blank","Not relevant","Slightly relevant","Moderately relevant","Highly relevant","Extremely relevant"))
```

##Cronbach's alpha##

To test Cronbach's alpha I redownloaded the GitHub data into another variable (because the alpha function requires interger variables and earlier in the code I turned the question 9 questions into factors with labels) and assigned the column names again. I then created a data frame of all the questions for question 9 and assigned it to the variable "Question9". I then calculated Cronbach's alpa.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
Cronbach_GitHub_data<-"https://raw.githubusercontent.com/PsychBrief/Why-do-psychologists-leave-academia/master/MyDataReport_(with_exclusions).csv"
Cronbach_QuestionnaireData<-read.csv(Cronbach_GitHub_data)
names(Cronbach_QuestionnaireData)<-c("Consent","Age","Work_Location","Sex","Occupation","Other1","Academic?","PhD","Postdoc","Assistant_Lecturer","Lecturer","Senior_Lecturer","Reader","Professor","Trainee_Prac_Psych","Prac_Psych","Other2","Left_Academia?","Stress","W_L_balance","Truth_of_field","Criticism_culture","Bullying","Pay","Criticised","Replics","Incentives","No_benefit","No_progress","Location","Funding","Other3","Comments")

Question9<-data.frame(Cronbach_QuestionnaireData$Stress, Cronbach_QuestionnaireData$W_L_balance, Cronbach_QuestionnaireData$Truth_of_field, Cronbach_QuestionnaireData$Criticism_culture, Cronbach_QuestionnaireData$Bullying, Cronbach_QuestionnaireData$Pay, Cronbach_QuestionnaireData$Criticised, Cronbach_QuestionnaireData$Replics, Cronbach_QuestionnaireData$Incentives, Cronbach_QuestionnaireData$No_benefit, Cronbach_QuestionnaireData$No_progress, Cronbach_QuestionnaireData$Location, Cronbach_QuestionnaireData$Funding)
library(coefficientalpha)
coefficientalpha::alpha(Question9)
```
With an alpha of 0.77, this is classed as "acceptable" but as Pegler, 2017 (https://adampegler.blogspot.co.uk/2017/02/behind-scenes-at-cronbachs-alpha.html) points out, Cronbach's alpha is limited so one shouldn't set too much store by it. 

## Chi-square tests of independence: ##

I then created a table with the variable "Sex" and the different reasons for leaving academia, allowing us to see whether there are any differences in response according to the sex of the participants. To test for a difference, chi-square tests were performed. Some respondents misunderstood what I was asking for when they had to give their sex or gave joke answers. In order for these participants to not bias the statistical analyses I performed, I excluded 8 participants. Leaving them in meant all the analyses comparing sex to another variable were non-significant. This data set can be found at: https://raw.githubusercontent.com/PsychBrief/Why-do-psychologists-leave-academia/master/MyDataReport_(with_full_sex_exclusions).csv The variables I focus on are "criticism_culture", "criticised", "truth_of_field", and "replics" as these are related to the motivating factor for this survey. To create the new data frame with the 8 exclusions I used the filter verb from the dplyr package.

```{r, echo=TRUE}
library("dplyr")
Sex_QuestionnaireData<-QuestionnaireData%>%filter(Sex=="m"|Sex=="f")
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Criticism_culture)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Criticism_culture)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Replics)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Replics)
```

In order not to inflate the p-values due to multiple comparisons, I used a Holm (1979) correction on the four p-values given when comparing the variables of interest to sex.

```{r, echo=TRUE}
Sex_pvalues<-c(0.007078, 0.01031, 0.1337, 0.8437)
p.adjust(Sex_pvalues, method = "holm", n = length(Sex_pvalues))
```

There was a significant difference between male and female participants on the variables relating to there being a culture of criticism within psychology and whether they had failed to replicate someone else's findings. This was true after controlling for multiple comparisons using the Holm (1979) correction. This means we can reject the null hypothesis that the participants rating of how relevant there being a culture of criticism and failing to replicate a finding is independent of their sex. Looking at the tables, we can see females were more likely to rate there being a culture of criticism as a "highly" or "extremely" relevant factor for them leaving/considering leaving academia, whereas men were more likely to give a higher rating than women on whether they had failed to successfully replicate a finding. 

```{r, echo=TRUE}
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Truth_of_field)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Truth_of_field)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Criticised)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Criticised)
```

There was no significant sex difference for either of the variables relating to being worried about how much of the literature was true and being criticised, so we cannot reject the null hypothesis that participants sex is independent of these variables.

I then analysed the rest of the variables to see if the participants sex was independent of the rankings given. After correcting for multiple testing, there were no significant differences. 

```{r, echo=TRUE}
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$No_progress)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$No_progress)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Stress)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Stress)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$W_L_balance)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$W_L_balance)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Bullying)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Bullying)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Pay)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Pay)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Incentives)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Incentives)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$No_benefit)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$No_benefit)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Location)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Location)
table(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Funding)
chisq.test(Sex_QuestionnaireData$Sex, Sex_QuestionnaireData$Funding)
Other_sexpvalues<-c(0.03204, 0.1852, 0.2594, 0.1193, 0.1237, 0.1538, 0.5339, 0.3074, 0.3903)
p.adjust(Other_sexpvalues, method = "holm", n = length(Other_sexpvalues))
```

To see whether the four variables of interest were independent of each other I performed a chi-square test on those variables and all were highly significant.

```{r, echo=TRUE}
chisq.test(QuestionnaireData$Criticised, QuestionnaireData$Truth_of_field)
chisq.test(QuestionnaireData$Criticised, QuestionnaireData$Replics)
chisq.test(QuestionnaireData$Criticism_culture, QuestionnaireData$Truth_of_field)
chisq.test(QuestionnaireData$Criticism_culture, QuestionnaireData$Replics)
```

##Mann-Whitney-Wilcoxon Test:##

To test for differences between the different variables in question 9, I analysed them using a Mann-Whitney-Wilcoxon Test. First I ranked the answers for the different variables.

```{r, echo=TRUE}
criticism_culture_ranked<-rank(QuestionnaireData$Criticism_culture)
truth_of_field_ranked<-rank(QuestionnaireData$Truth_of_field)
replics_ranked<-rank(QuestionnaireData$Replics)
criticised_ranked<-rank(QuestionnaireData$Criticised)
```

I then created an individual 2x2 table for each of the 4 variables and performed a Mann-Whitney-Wilcoxon Test on each.

```{r, echo=TRUE}
table(truth_of_field_ranked, criticism_culture_ranked)
wilcox.test(truth_of_field_ranked, criticism_culture_ranked)
table(truth_of_field_ranked, criticised_ranked)
wilcox.test(truth_of_field_ranked, criticised_ranked)
table(replics_ranked, criticism_culture_ranked)
wilcox.test(replics_ranked, criticism_culture_ranked)
table(replics_ranked, criticised_ranked)
wilcox.test(replics_ranked, criticised_ranked)
MWTest_pvalues<-c(0.4144, 0.2113, 0.0003285, 2.2e-16)
p.adjust(MWTest_pvalues, method = "holm", n = length(MWTest_pvalues))
```

We can see that the results for the Mann-Whitney-Wilcoxon Test were mixed: half were significant at the 0.05 significance level and half were not. The tests involving the variables failure to replicate and being criticised and failure to replicate and the culture of criticism are significantly different. This means we can reject the null hypothesis that the population distributions aren't identical/that they are rating the reasons as equally relevant. Checking the table we can see that a culture of criticism was ranked as a more relevant factor for leaving or considering leaving academia than failing to replicate. Looking at failing to replicate against being criticised, failing to replicate was ranked as more relevant. So of the three variables that were significantly different: culture of criticism was ranked the highest, then failure to replicate, and finally being criticised. 

Looking at the first graph, we can see the more traditional reasons for leaving academia were rated as higher (desire for a better work-life balance, the large amount of effort required for funding, and the lack of incentives). We can test whether they significantly differ from our variables of interest.

```{r, echo=TRUE}
Incentives_ranked<-rank(QuestionnaireData$Incentives)
W_L_Balance_ranked<-rank(QuestionnaireData$W_L_balance)
Funding_ranked<-rank(QuestionnaireData$Funding)
table(criticised_ranked, Incentives_ranked)
wilcox.test(criticised_ranked, Incentives_ranked)
table(criticism_culture_ranked, Incentives_ranked)
wilcox.test(criticism_culture_ranked, Incentives_ranked)
table(truth_of_field_ranked, Incentives_ranked)
wilcox.test(truth_of_field_ranked, Incentives_ranked)
table(replics_ranked, Incentives_ranked)
wilcox.test(replics_ranked, Incentives_ranked)
table(criticised_ranked, W_L_Balance_ranked)
wilcox.test(criticised_ranked, W_L_Balance_ranked)
table(criticism_culture_ranked, W_L_Balance_ranked)
wilcox.test(criticism_culture_ranked, W_L_Balance_ranked)
table(truth_of_field_ranked, W_L_Balance_ranked)
wilcox.test(truth_of_field_ranked, W_L_Balance_ranked)
table(replics_ranked, W_L_Balance_ranked)
wilcox.test(replics_ranked, W_L_Balance_ranked)
table(criticised_ranked, Funding_ranked)
wilcox.test(criticised_ranked, Funding_ranked)
table(criticism_culture_ranked, Funding_ranked)
wilcox.test(criticism_culture_ranked, Funding_ranked)
table(truth_of_field_ranked, Funding_ranked)
wilcox.test(truth_of_field_ranked, Funding_ranked)
table(replics_ranked, Funding_ranked)
wilcox.test(replics_ranked, Funding_ranked)
Other_MWTest_pvalues<-c(0.004731, 0.6643, 0.6587, 0.083, 0.3113, 0.2017, 0.225, 0.8702, 0.7671, 0.6136, 0.05261, 0.5335)
p.adjust(Other_MWTest_pvalues, method = "holm", n = length(Other_MWTest_pvalues))
```

Prior to the Holm (1979) correction, the only two variables significantly different are being harshly criticised and the lack of proper incentives. However, after the correction it is just statistically non-significant. All other tests were non-significant. 

##Spearman's Rho##

To see if there is a correlation between these ranked variables, I calculated Spearman's Rho for each pair of variables. Each pair of correlations was significant with a small positive correlation found.

```{r, echo=TRUE}
cor.test(truth_of_field_ranked, criticism_culture_ranked)
cor.test(truth_of_field_ranked, criticised_ranked)
cor.test(replics_ranked, criticism_culture_ranked)
cor.test(replics_ranked, criticised_ranked)
```
